# This inputs the needed function
source("utilities_keras3.R")
shapes.func = function(X){
1/(1+exp(-(1-5*X[,1]*X[,2])))
}
scales.func = function(X){
5*(1-1/(1+exp(-(1-5*X[,1]*X[,2]))))
}
xi_custom_activation <- function(x) {
0.5 * activation_sigmoid(x[all_dims(),1:1]) #(0,0.5)
# 0.5 * activation_tanh(x[all_dims(),1:1]) +0.1 #(-0.4,0.6)
}
generate.simulation.datset <- function(n = 1000, n.knots = 25)
{
x <- runif(3*n, 0, 1)
x <- matrix(x,nrow = n,ncol = 3)
shapes= shapes.func(x)
scales= scales.func(x)
y <- apply(cbind(scales,shapes),1,function(x) rlnorm(1,x[1],x[2]))
n_validation <- n
x_validation <- runif(3*n_validation, 0, 1)
x_validation <- matrix(x_validation,nrow = n_validation, ncol = 3)
shapes_validation= shapes.func(x_validation)
scales_validation= scales.func(x_validation)
y_validation <- apply(cbind(scales_validation,shapes_validation),1,function(x) rlnorm(1,x[1],x[2]))
min.Y=0
max.Y=max(y, y_validation)
y <- (y-min.Y)/(max.Y-min.Y)
y <- pmin(pmax(y, 1e-8), 1 - 1e-8)
#knots = quantile(y,probs=seq(1/(n.knots-2), 1-1/(n.knots-2), length=n.knots-3))
knots = seq(1/(n.knots-2), 1-1/(n.knots-2), length=n.knots-3)
m_basis_training <- t(basis(y,n.knots,knots))
i_basis_training <- t(basis(y,n.knots,knots, integral = TRUE))
n.seq = 1001
#y.seq<-c(0,exp(seq(log(1e-10), log(1), length.out = n.seq)))
y.seq <- seq(0,1,length=n.seq)
F.basis.seq <- tf$constant(basis(y.seq , n.knots,knots,integral = TRUE),dtype = 'float32') #this is used later to get quantiles
f.basis.seq <- tf$constant(basis(y.seq , n.knots,knots),dtype = 'float32')
y.seq <- tf$constant(y.seq, dtype = 'float32')
y <- matrix(y,nrow = n)
y_validation <- (y_validation-min.Y)/(max.Y-min.Y)
m_basis_validation <- t(basis(y_validation,n.knots,knots))
i_basis_validation <- t(basis(y_validation,n.knots,knots, integral = TRUE))
y_validation <- matrix(y_validation,nrow = n_validation)
############ test data
set.seed(1)
n_test <- 5000
x_testing <- runif(3*n_test, 0, 1)
x_testing <- matrix(x_testing,nrow = n_test, ncol = 3)
#Y_test <- rnorm(n_test, X_test, 0.8)
shapes_test= shapes.func(x_testing)
scales_test= scales.func(x_testing)
y_testing <-  apply(cbind(scales_test,shapes_test),1,function(x) rlnorm(1,x[1],x[2]))
y_testing <- (y_testing-min.Y)/(max.Y-min.Y)
m_basis_testing <- t(basis(y_testing,n.knots,knots))
i_basis_testing <- t(basis(y_testing,n.knots, knots, integral = TRUE))
y_testing <- matrix(y_testing,nrow = n_test)
return (list(x_training = x, y_training = y, m_basis_training = m_basis_training, i_basis_training = i_basis_training,
x_validation = x_validation, y_validation = y_validation, m_basis_validation = m_basis_validation,
i_basis_validation = i_basis_validation, x_testing = x_testing, y_testing = y_testing, m_basis_testing = m_basis_testing,
i_basis_testing = i_basis_testing, knots = knots))
}
# This the SPQRX model to be
SPQRX <- function(input_dim, hidden_dim, k) {
input_cov <- keras_input(shape = input_dim, name = "covariates")
input_y   <- keras_input(shape = 1, name = "data")
input_I   <- keras_input(shape = k, name = "I_basis")
x <- input_cov
for (h in hidden_dim) {
x <- layer_dense(x, units = h, activation = "relu")
}
probs <- layer_dense(x, units = k, activation = "softmax", name = "probs")
xi    <- layer_dense(x, units = 1, activation = xi_custom_activation, name = "xi")
output <- keras3::op_concatenate(
list(probs, xi, input_y, input_I),
axis = -1
) |> layer_identity(name = "outs")
keras_model(
inputs  = list(input_cov, input_y, input_I),
outputs = output,
name    = "SPQR.heavy"
)
}
testing_spqrx <- function(input_dim, hidden_dim, k) {
stopifnot(is.numeric(hidden_dim), length(hidden_dim) >= 1)
input_cov <- keras_input(shape = input_dim, name = "covariates")
input_y   <- keras_input(shape = 1, name = "data")
input_I   <- keras_input(shape = k, name = "I_basis")
x <- keras3::op_concatenate(
list(input_cov, input_y, input_I),
axis = -1
)
for (h in hidden_dim) {
x <- layer_dense(x, units = h, activation = "relu")
}
probs.heavy <- layer_dense(
x, k,
activation = "softmax",
name = "probs.heavy"
)
shap.heavy <- layer_dense(
x, 1,
activation = xi_custom_activation,
name = "xi.heavy",
kernel_regularizer = keras3::regularizer_l1(0.01)
)
# âœ… keras3-safe concatenation
outs <- keras3::op_concatenate(
list(probs.heavy, shap.heavy),
axis = -1
) |> layer_identity(name = "outs.heavy")
model <- keras_model(
inputs  = list(input_cov, input_y, input_I),
outputs = outs,
name    = "SPQR.heavy"
)
return (model)
}
SPQR <- function(input_dim, hidden_dim, k) {
stopifnot(is.numeric(hidden_dim), length(hidden_dim) >= 1)
input_cov <- keras_input(shape = input_dim, name = "covariates")
input_y   <- keras_input(shape = 1, name = "data")
input_I   <- keras_input(shape = k, name = "I_basis")
x <- input_cov
for (h in hidden_dim) {
x <- x |> layer_dense(units = h, activation = "relu")
}
probs <- x |> layer_dense(units = k, activation = "softmax", name = "probs")
xi    <- x |> layer_dense(units = 1, activation = xi_custom_activation, name = "xi")
output <- keras3::op_concatenate(
list(probs, xi, input_y, input_I),
axis = -1
) |> layer_identity(name = "outs")
keras_model(
inputs  = list(input_cov, input_y, input_I),
outputs = output,
name    = "SPQR"
)
}
initialize_spqrx_from_spqr <- function(spqrx, spqr) {
spqr_layers  <- spqr$layers
spqrx_layers <- spqrx$layers
for (i in seq_along(spqrx_layers)) {
l_new <- spqrx_layers[[i]]
l_old <- spqr_layers[[i]]
if (!inherits(l_new, "keras.layers.Dense")) next()
if (!inherits(l_old, "keras.layers.Dense")) next()
w <- l_old$get_weights()
if (length(w) == 0) next()
# dimension check (important for stability)
if (!all(dim(w[[1]]) == dim(l_new$get_weights()[[1]]))) next()
l_new$set_weights(w)
}
spqrx
}
train_spqr <- function(spqr_model, x_training, y_training, i_basis_training,
x_validation, y_validation, i_basis_validation, m_basis_validation)
{
model |> compile(
loss = nloglik_loss_SPQR,
optimizer = optimizer_adam(learning_rate = 0.001)
)
checkpoint <- callback_model_checkpoint(filepath=paste0('runs/',array.id,'/spqr_initial.weights.h5'), monitor = "val_loss", verbose = 0,
save_best_only = TRUE, save_weights_only = TRUE, mode = "min",
save_freq = "epoch")
history <- model |> fit(
list(covariates = x_training, data = y_training, I_basis = i_basis_training),
M_basis,
epochs = 200,
batch_size = 32,
callbacks=list(checkpoint,callback_early_stopping(monitor = "val_loss",
min_delta = 0, patience = 5)),
validation_data=list(
list(covariates = x_validation, data = y_validation, I_basis = i_basis_validation),
m_basis_validation)
)
return (model)
}
simulation_dataset <- generate.simulation.datset(10000, k)
x_training <- simulation_dataset$x_training
y_training <- simulation_dataset$y_training
m_basis_training <- simulation_dataset$m_basis_training
i_basis_training <- simulation_dataset$i_basis_training
x_validation <- simulation_dataset$x_validation
y_validation <- simulation_dataset$y_validation
m_basis_validation <- simulation_dataset$m_basis_validation
i_basis_validation <- simulation_dataset$i_basis_validation
x_testing <- simulation_dataset$x_testing
y_testing <- simulation_dataset$y_testing
m_basis_testing <- simulation_dataset$m_basis_testing
i_basis_testing <- simulation_dataset$i_basis_testing
knots <- simulation_dataset$knots
model <- SPQR(3, c(20, 20, 20), k)
model.heavy <- SPQRX(input_dim = 3 , hidden_dim = c(20 , 20 , 20), k = k  )
c1=5
c2=25
n.seq = 1001
#y.seq <- seq(0,1,length=n.seq)
y.seq <- seq(1e-8, 1-1e-8, length=n.seq)
F.basis.seq <- tf$constant(basis(y.seq , k,knots,integral = TRUE),dtype = 'float32') #this is used later to get quantiles
f.basis.seq <- tf$constant(basis(y.seq , k ,knots),dtype = 'float32')
y.seq <- tf$constant(y.seq, dtype = 'float32')
model.heavy |> compile(
loss = nloglik_loss(F.basis.seq,
f.basis.seq,
y.seq=y.seq,
p_a=p_a,p_b=p_b,
c1=c1,c2=c2,
lambda=lambda),
optimizer = optimizer_adam()
)
# This inputs the needed function
source("utilities_keras3.R")
# Generating arguments matrix ---------------------------------------------
# Fixed arguments
Sys.setenv(CUDA_VISIBLE_DEVICES = "-1")
test.nums = 1:100
ns = c(1000,10000)
p_as=c(0.75,0.9,0.925)
p_bs=c(0.95,0.99,0.999)
knots = c(15,20,25)
units = c(16,32)
c1 = c(5,25)
args_matrix = expand.grid(test.nums,ns,p_as,p_bs,knots,units,c1)
args_matrix = cbind(args_matrix,1:dim(args_matrix)[1])
dim(args_matrix)
saveRDS(args_matrix,file="custom_args_matrix.rds")
#array.id=commandArgs(trailingOnly=T) #Assuming job array submission with bash/slurm
array.id=2
print(array.id)
args=args_matrix[as.numeric(array.id),]
print(args)
#First argument determines the test number and the random seed for generating data
test.num = as.numeric(args[1])
#Second argument determines the sample size of the data
n = as.numeric(args[2])
#Third argument determines p_a
p_a = as.numeric(args[3])
#Fourth argument determines p_b
p_b = as.numeric(args[4])
#Fifth argument determines number of knots
n.knots = as.numeric(args[5])
#Sixth argument determines number of knots
units = as.numeric(args[6])
#Seventh argument determines c1
c1 = as.numeric(args[7])
#Finally, the eighth argument gives the array ID of the job. This can be used as the reference for a particular combination of parameters
array.id = as.numeric(args[8])
library(fields)
library(keras3)
library(GpGp)
library(evd)
library(LaplacesDemon)
tf <- reticulate::import("tensorflow")
source("utilities_keras3.R")
model |> compile(
loss = nloglik_loss_SPQR,
optimizer = optimizer_adam(learning_rate = 0.001)
)
model |> compile(
loss = nloglik_loss_SPQR,
optimizer = optimizer_adam(learning_rate = 0.001)
)
#Initial SPQR fit
model <- SPQRX(input_dim = 3, hidden_dim = c(20), k = n.knots)
source("utilities_keras3.R")
############ training data
set.seed(test.num)
X <- runif(3*n, 0, 1)
X <- matrix(X,nrow = n,ncol = 3)
shapes.func = function(X){
1/(1+exp(-(1-5*X[,1]*X[,2])))
}
scales.func = function(X){
5*(1-1/(1+exp(-(1-5*X[,1]*X[,2]))))
}
generate.simulation.dataset <- function(n = 1000, n.knots = 25)
{
x <- runif(3*n, 0, 1)
x <- matrix(x,nrow = n,ncol = 3)
shapes= shapes.func(x)
scales= scales.func(x)
y <- apply(cbind(scales,shapes),1,function(x) rlnorm(1,x[1],x[2]))
n_validation <- n
x_validation <- runif(3*n_validation, 0, 1)
x_validation <- matrix(x_validation,nrow = n_validation, ncol = 3)
shapes_validation= shapes.func(x_validation)
scales_validation= scales.func(x_validation)
y_validation <- apply(cbind(scales_validation,shapes_validation),1,function(x) rlnorm(1,x[1],x[2]))
min.Y=0
max.Y=max(y, y_validation)
y <- (y-min.Y)/(max.Y-min.Y)
y <- pmin(pmax(y, 1e-8), 1 - 1e-8)
#knots = quantile(y,probs=seq(1/(n.knots-2), 1-1/(n.knots-2), length=n.knots-3))
knots = seq(1/(n.knots-2), 1-1/(n.knots-2), length=n.knots-3)
m_basis_training <- t(basis(y,n.knots,knots))
i_basis_training <- t(basis(y,n.knots,knots, integral = TRUE))
n.seq = 1001
#y.seq<-c(0,exp(seq(log(1e-10), log(1), length.out = n.seq)))
y.seq <- seq(0,1,length=n.seq)
F.basis.seq <- tf$constant(basis(y.seq , n.knots,knots,integral = TRUE),dtype = 'float32') #this is used later to get quantiles
f.basis.seq <- tf$constant(basis(y.seq , n.knots,knots),dtype = 'float32')
y.seq <- tf$constant(y.seq, dtype = 'float32')
y <- matrix(y,nrow = n)
y_validation <- (y_validation-min.Y)/(max.Y-min.Y)
m_basis_validation <- t(basis(y_validation,n.knots,knots))
i_basis_validation <- t(basis(y_validation,n.knots,knots, integral = TRUE))
y_validation <- matrix(y_validation,nrow = n_validation)
############ test data
set.seed(1)
n_test <- 5000
x_testing <- runif(3*n_test, 0, 1)
x_testing <- matrix(x_testing,nrow = n_test, ncol = 3)
#Y_test <- rnorm(n_test, X_test, 0.8)
shapes_test= shapes.func(x_testing)
scales_test= scales.func(x_testing)
y_testing <-  apply(cbind(scales_test,shapes_test),1,function(x) rlnorm(1,x[1],x[2]))
y_testing <- (y_testing-min.Y)/(max.Y-min.Y)
m_basis_testing <- t(basis(y_testing,n.knots,knots))
i_basis_testing <- t(basis(y_testing,n.knots, knots, integral = TRUE))
y_testing <- matrix(y_testing,nrow = n_test)
return (list(x_training = x, y_training = y, m_basis_training = m_basis_training, i_basis_training = i_basis_training,
x_validation = x_validation, y_validation = y_validation, m_basis_validation = m_basis_validation,
i_basis_validation = i_basis_validation, x_testing = x_testing, y_testing = y_testing, m_basis_testing = m_basis_testing,
i_basis_testing = i_basis_testing, knots = knots))
}
SPQRX <- function(input_dim, hidden_dim, k) {
input_cov <- keras_input(shape = input_dim, name = "covariates")
input_y   <- keras_input(shape = 1, name = "data")
input_I   <- keras_input(shape = k, name = "I_basis")
x <- input_cov
for (h in hidden_dim) {
x <- layer_dense(x, units = h, activation = "relu")
}
probs <- layer_dense(x, units = k, activation = "softmax", name = "probs")
xi    <- layer_dense(x, units = 1, activation = xi_custom_activation, name = "xi")
output <- keras3::op_concatenate(
list(probs, xi, input_y, input_I),
axis = -1
) |> layer_identity(name = "outs")
keras_model(
inputs  = list(input_cov, input_y, input_I),
outputs = output,
name    = "SPQR.heavy"
)
}
xi_custom_activation <- function(x) {
0.5 * activation_sigmoid(x[all_dims(),1:1]) #(0,0.5)
# 0.5 * activation_tanh(x[all_dims(),1:1]) +0.1 #(-0.4,0.6)
}
sim_data <- generate.simulation.dataset(7000, n.knots = n.knots)
x_training <- sim_data$x_training
y_training <- sim_data$y_training
m_basis_training <- sim_data$m_basis_training
i_basis_training <- sim_data$i_basis_training
x_validation <- sim_data$x_validation
y_validation <- sim_data$y_validation
m_basis_validation <- sim_data$m_basis_validation
i_basis_validation <- sim_data$i_basis_validation
x_testing <- sim_data$x_testing
y_testing <- sim_data$y_testing
m_basis_testing <- sim_data$m_basis_testing
i_basis_testing <- sim_data$i_basis_testing
#Initial SPQR fit
model <- SPQRX(input_dim = 3, hidden_dim = c(20), k = n.knots)
model |> compile(
loss = nloglik_loss_SPQR,
optimizer = optimizer_adam(learning_rate = 0.001)
)
source("utilities_keras3.R")
setwd('/home/dalton/SPQRx/R')
source('SPQRX.R')
source('utilities_keras3.R')
dataset <- read.csv('/home/dalton/Downloads/housing (1).csv')
coords <- strsplit(gsub("POINT \\(|\\)", "", dataset$geometry), " ")
# Convert to numeric matrix
coords <- do.call(rbind, lapply(coords, function(x) as.numeric(x)))
# Assign to new columns
dataset$lon <- coords[,1]
dataset$lat <- coords[,2]
y <- matrix ( dataset$price, ncol = 1)
y <- y
dataset <- dataset[, names(dataset) != "price"]
x <- as.matrix(dataset[, c('bedrooms', 'bathrooms', 'squareFeet', 'lon', 'lat')])
data <- preprocessing.data(x, y, n.knots = 25)
x_training <- data$x_training
x_validation <- data$x_validation
x_testing <- data$x_testing
y_training <- data$y_training
y_validation <- data$y_validation
y_testing <- data$y_testing
p_a = 0.95
p_b = 0.999
c1 = 3
c2 = 5
hyperparameter <- create.packages.hyperparameter(
p_a = 0.95,
p_b = 0.990,
c1 = 35,
c2 = 5,
batch_size = 50)
model.heavy <- fit_spqrx(input_dim = 5, hidden_dim = c(30, 30, 30), n.knots = 25, x_training = x_training, x_validation = x_validation,
y_training = y_training, y_validation = y_validation, hyperparameter = hyperparameter, spqrx = TRUE)
model.heavy <- fit_spqrx(input_dim = 5, hidden_dim = c(30, 30, 30), n.knots = 25, x_training = x_training, x_validation = x_validation,
y_training = y_training, y_validation = y_validation, hyperparameter = hyperparameter, spqrx = TRUE)
source('SPQRX.R')
model.heavy <- fit_spqrx(input_dim = 5, hidden_dim = c(30, 30, 30), n.knots = 25, x_training = x_training, x_validation = x_validation,
y_training = y_training, y_validation = y_validation, hyperparameter = hyperparameter, spqrx = TRUE)
cdf_values <- predict_spqrx(model.heavy , x_training, y_training, type = 'QF', tau = 0.95)
cdf_values
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
eval.plot.QVI(model.heavy, x_training , var.indexs = c(1,  3, 4, 5))
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
source('SPQRX.R')
source('utilities_keras3.R')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
print(ab)
ab
ab
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
source('SPQRX.R')
source('utilities_keras3.R')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
abort
ab
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
dataset <- read.csv('/home/dalton/Downloads/housing (1).csv')
dataset <- read.csv('/home/dalton/Downloads/housing (1).csv')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
source('SPQRX.R')
source('utilities_keras3.R')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
ab
ab
source('SPQRX.R')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
source('utilities_keras3.R')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
source('SPQRX.R')
source('utilities_keras3.R')
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
setwd('/home/dalton/SPQRx/R')
source('SPQRX.R')
source('utilities_keras3.R')
dataset <- read.csv('/home/dalton/Downloads/housing (1).csv')
coords <- strsplit(gsub("POINT \\(|\\)", "", dataset$geometry), " ")
# Convert to numeric matrix
coords <- do.call(rbind, lapply(coords, function(x) as.numeric(x)))
# Assign to new columns
dataset$lon <- coords[,1]
dataset$lat <- coords[,2]
y <- matrix ( dataset$price, ncol = 1)
y <- y
dataset <- dataset[, names(dataset) != "price"]
x <- as.matrix(dataset[, c('bedrooms', 'bathrooms', 'squareFeet', 'lon', 'lat')])
data <- preprocessing.data(x, y, n.knots = 25)
x_training <- data$x_training
x_validation <- data$x_validation
x_testing <- data$x_testing
y_training <- data$y_training
y_validation <- data$y_validation
y_testing <- data$y_testing
p_a = 0.95
p_b = 0.999
c1 = 3
c2 = 5
hyperparameter <- create.packages.hyperparameter(
p_a = 0.95,
p_b = 0.990,
c1 = 35,
c2 = 5,
batch_size = 50)
model.heavy <- fit_spqrx(input_dim = 5, hidden_dim = c(30, 30, 30), n.knots = 25, x_training = x_training, x_validation = x_validation,
y_training = y_training, y_validation = y_validation, hyperparameter = hyperparameter, spqrx = TRUE)
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
ab
ab
dim(ab)
debug(predict.spqrk.GPD)
lime_analysis <- eval.explain.lime(model.heavy, x_training, x_testing[c(10: 20), ])
setwd('/home/dalton/SPQRx/R')
source('SPQRX.R')
source('utilities_keras3.R')
dataset <- read.csv('/home/dalton/Downloads/housing (1).csv')
coords <- strsplit(gsub("POINT \\(|\\)", "", dataset$geometry), " ")
# Convert to numeric matrix
coords <- do.call(rbind, lapply(coords, function(x) as.numeric(x)))
# Assign to new columns
dataset$lon <- coords[,1]
dataset$lat <- coords[,2]
y <- matrix ( dataset$price, ncol = 1)
y <- y
dataset <- dataset[, names(dataset) != "price"]
x <- as.matrix(dataset[, c('bedrooms', 'bathrooms', 'squareFeet', 'lon', 'lat')])
data <- preprocessing.data(x, y, n.knots = 25)
x_training <- data$x_training
x_validation <- data$x_validation
x_testing <- data$x_testing
y_training <- data$y_training
y_validation <- data$y_validation
y_testing <- data$y_testing
p_a = 0.95
p_b = 0.999
c1 = 3
c2 = 5
hyperparameter <- create.packages.hyperparameter(
p_a = 0.95,
p_b = 0.990,
c1 = 35,
c2 = 5,
batch_size = 50)
